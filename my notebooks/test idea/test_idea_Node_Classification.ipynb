{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e_9AUumjwjb4",
        "wwfgmk5hWuNQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "My idea is breaking each strong node to multiple nodes!"
      ],
      "metadata": {
        "id": "NocASKQXwwK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX"
      ],
      "metadata": {
        "id": "GAA6qrdUuw28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hide"
      ],
      "metadata": {
        "id": "e_9AUumjwjb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "OABQtnq0Yg9-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "2Hjurjtbxq0W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1op-CbyLuN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89ea2ad-cd59-4a25-ff02-46c77c3e4d45"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 25.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 26.8 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GATConv"
      ],
      "metadata": {
        "id": "8c3_5dBORIAX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszt2RUHE7lW"
      },
      "source": [
        "## Node Classification with Graph Neural Networks\n",
        "\n",
        "[Previous: Introduction: Hands-on Graph Neural Networks](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8)\n",
        "\n",
        "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
        "\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total).\n",
        "\n",
        "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
        "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGrKO5YH11-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a93635-0c98-4231-9ad0-a43dec00eb98"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "datasetnames = ['PubMed','Cora','CiteSeer']\n",
        "datasetname = datasetnames[2] \n",
        "dataset = Planetoid(root='data/Planetoid', name=datasetname, transform=NormalizeFeatures())\n",
        "# dataset = Planetoid(root='data/Planetoid', name='CiteSeer', transform=NormalizeFeatures())\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: CiteSeer():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 3703\n",
            "Number of classes: 6\n",
            "\n",
            "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
            "===========================================================================================================\n",
            "Number of nodes: 3327\n",
            "Number of edges: 9104\n",
            "Average node degree: 2.74\n",
            "Number of training nodes: 120\n",
            "Training node label rate: 0.04\n",
            "Has isolated nodes: True\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hide now, Test"
      ],
      "metadata": {
        "id": "x5FST8azxIEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "0UR8MRliw2yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3c4034-679e-4ae2-e9c7-879f02fc42a4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    log['dataset']=dataset\n",
        "except:\n",
        "    log = {}\n",
        "    log['dataset']=dataset"
      ],
      "metadata": {
        "id": "qH6zZsOlxZDQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = np.zeros(len(data.x))"
      ],
      "metadata": {
        "id": "WGhxfEilxZJ-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data.edge_index[0])):\n",
        "    a =  data.edge_index[0][i]\n",
        "    b =  data.edge_index[1][i]\n",
        "    nodes[a]+=1\n",
        "    nodes[b]+=1"
      ],
      "metadata": {
        "id": "b_lqkQymxZLm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuM5xGCvxI2x",
        "outputId": "a682781b-95ac-4005-b044-b86f294beac2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198.0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_nodes = np.copy(nodes)"
      ],
      "metadata": {
        "id": "IkdtuhxBk8Et"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(nodes, bins='auto')  # arguments are passed to np.histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "GITTLPoQxI9Z",
        "outputId": "a6c9a05b-c546-45a0-fc13-9d91073bfd2c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes2 = nodes[nodes>50]\n",
        "len(nodes2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIJDLesrylep",
        "outputId": "0d93cc54-ce22-47d2-c65b-9104ec01db9d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(nodes2, bins='auto')  # arguments are passed to np.histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "SLR0Wrcuylfx",
        "outputId": "091b7961-78d2-4f7f-8a17-73e37e4fabe4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I guess break each extra 100 to a new part"
      ],
      "metadata": {
        "id": "Odg-jMOXy9fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "EoY88IQCQCCP"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log['raw data'] = str(data)"
      ],
      "metadata": {
        "id": "7PnIsPqvZj8d"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.x\n",
        "ei = data.edge_index \n",
        "y = data.y\n",
        "train_mask, val_mask, test_mask = data.train_mask, data.val_mask, data.test_mask"
      ],
      "metadata": {
        "id": "7zgnIWHXylnh"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiappender(l,e):\n",
        "    d = e.reshape(1,-1)\n",
        "    l = torch.cat([l, torch.tensor(d)], dim=0)\n",
        "    return l"
      ],
      "metadata": {
        "id": "WrSjKYPP_7P2"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def appender(l,e):\n",
        "    z = torch.cat([l, torch.tensor(e).reshape(1)], dim=0)\n",
        "    return z"
      ],
      "metadata": {
        "id": "UWL0o86IAwFz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log['info'] = {}\n",
        "# log['info']['#of(edge_index,x,ei,train_mask']=[len(data.edge_index[0]),len(data.x),len(ei),len(train_mask)]"
      ],
      "metadata": {
        "id": "GAddtif-wxdD"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run phase 1"
      ],
      "metadata": {
        "id": "OSa--NdYVuWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save original data:"
      ],
      "metadata": {
        "id": "KP0kydxvVw15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CGraph:\n",
        "\n",
        "\t# init function to declare class variables\n",
        "\tdef __init__(self, V):\n",
        "\t\tself.V = V\n",
        "\t\tself.adj = [[] for i in range(V)]\n",
        "\n",
        "\tdef DFSUtil(self, temp, v, visited):\n",
        "\n",
        "\t\t# Mark the current vertex as visited\n",
        "\t\tvisited[v] = True\n",
        "\n",
        "\t\t# Store the vertex to list\n",
        "\t\ttemp.append(v)\n",
        "\n",
        "\t\t# Repeat for all vertices adjacent\n",
        "\t\t# to this vertex v\n",
        "\t\tfor i in self.adj[v]:\n",
        "\t\t\tif visited[i] == False:\n",
        "\n",
        "\t\t\t\t# Update the list\n",
        "\t\t\t\ttemp = self.DFSUtil(temp, i, visited)\n",
        "\t\treturn temp\n",
        "\n",
        "\t# method to add an undirected edge\n",
        "\tdef addEdge(self, v, w):\n",
        "\t\tself.adj[v].append(w)\n",
        "\t\tself.adj[w].append(v)\n",
        "\n",
        "\t# Method to retrieve connected components\n",
        "\t# in an undirected graph\n",
        "\tdef connectedComponents(self):\n",
        "\t\tvisited = []\n",
        "\t\tcc = []\n",
        "\t\tfor i in range(self.V):\n",
        "\t\t\tvisited.append(False)\n",
        "\t\tfor v in range(self.V):\n",
        "\t\t\tif visited[v] == False:\n",
        "\t\t\t\ttemp = []\n",
        "\t\t\t\tcc.append(self.DFSUtil(temp, v, visited))\n",
        "\t\treturn cc"
      ],
      "metadata": {
        "id": "wDO5L_pjC4d_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findcomponents(data):\n",
        "    \n",
        "    x = data.x\n",
        "    ei = data.edge_index\n",
        "\n",
        "    cg = CGraph(len(x))\n",
        "\n",
        "    for i in tqdm(range(len(ei[0]))):\n",
        "        cg.addEdge(ei[0][i],ei[1][i])\n",
        "        cg.addEdge(ei[1][i],ei[0][i])\n",
        "\n",
        "\n",
        "    cc = cg.connectedComponents()\n",
        "   \n",
        "    return cc"
      ],
      "metadata": {
        "id": "SG1WsFqOC4WQ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insertnoise(data,noiseratio):\n",
        "    x = data.x\n",
        "    ei = data.edge_index \n",
        "    train_mask, val_mask, test_mask = data.train_mask, data.val_mask, data.test_mask\n",
        "\n",
        "    cc = findcomponents(data)\n",
        "    \n",
        "    lennoise = int(len(ei[0]) * noiseratio)\n",
        "    for i in range(lennoise):\n",
        "        n1 = random.choice(random.choice(cc))\n",
        "        n2 = random.choice(random.choice(cc))\n",
        "        e = np.array([n1,n2])\n",
        "        d = e.reshape(-1,1)\n",
        "        ei = torch.cat([ei, torch.tensor(d)], dim=1)      \n",
        "\n",
        "\n",
        "    data.x = x\n",
        "    data.edge_index = ei \n",
        "    data.train_mask, data.val_mask, data.test_mask = train_mask, val_mask, test_mask\n",
        "    return data"
      ],
      "metadata": {
        "id": "AC23AMvTC4Ng"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data_ready(flag_type_2,\n",
        "                    each_part_size,\n",
        "                    istype2 = False,\n",
        "                    noiseratio = 0.001):\n",
        "    \n",
        "    dataset = Planetoid(root='data/Planetoid', name=datasetname, transform=NormalizeFeatures())\n",
        "    data = dataset[0]\n",
        "    \n",
        "    redunclass = [[i] for i in range(len(data.x))]\n",
        "\n",
        "    nodes = np.zeros(len(data.x))\n",
        "\n",
        "\n",
        "    for i in range(len(data.edge_index[0])):\n",
        "        a =  data.edge_index[0][i]\n",
        "        b =  data.edge_index[1][i]\n",
        "        nodes[a]+=1\n",
        "        nodes[b]+=1\n",
        "    \n",
        "\n",
        "    strongs = []\n",
        "    for i in range(len(nodes)):\n",
        "        if nodes[i]>each_part_size:\n",
        "            strongs.append(i)\n",
        "    \n",
        "    if noiseratio>0:\n",
        "        data = insertnoise(data,noiseratio)\n",
        "    \n",
        "    x = data.x\n",
        "    ei = data.edge_index \n",
        "    y = data.y\n",
        "    train_mask, val_mask, test_mask = data.train_mask, data.val_mask, data.test_mask\n",
        "\n",
        "    d = False  \n",
        "\n",
        "    for s in strongs:\n",
        "        n = nodes[s]\n",
        "        while(n>each_part_size):\n",
        "\n",
        "            x = multiappender(x,x[s])\n",
        "\n",
        "            redunclass[s].append(len(x)-1)\n",
        "            y = appender(y,y[s])\n",
        "\n",
        "            train_mask = appender(train_mask,train_mask[s])\n",
        "            val_mask = appender(val_mask,val_mask[s]) \n",
        "            \n",
        "            if istype2:\n",
        "                test_mask = appender(test_mask,False)\n",
        "                test_mask[s] = False\n",
        "            else:\n",
        "                test_mask = appender(test_mask,test_mask[s])\n",
        "            \n",
        "            if n >= each_part_size*1.5:\n",
        "                t = each_part_size\n",
        "                n-=t\n",
        "                \n",
        "                for i in range(len(ei[0])):\n",
        "                    e1,e2 = ei[0][i],ei[1][i]\n",
        "                    \n",
        "                    if t<=0:\n",
        "                        break\n",
        "\n",
        "                    if e1==s:\n",
        "                        t-=1\n",
        "                        ei[0][i] = len(x)-1\n",
        "                    elif e2==s:\n",
        "                        t-=1\n",
        "                        ei[1][i] = len(x)-1\n",
        "                    \n",
        "            else:\n",
        "                t = int((n+1)/2)\n",
        "                n-=t\n",
        "                \n",
        "                for i in range(len(ei[0])):\n",
        "                    if t<=0:\n",
        "                        break\n",
        "\n",
        "                    e1,e2 = ei[0][i],ei[1][i]\n",
        "                    if e1==s:\n",
        "                        t-=1\n",
        "                        ei[0][i] = len(x)-1\n",
        "                    elif e2==s:\n",
        "                        t-=1\n",
        "                        ei[1][i] = len(x)-1\n",
        "                    \n",
        "            if flag_type_2:\n",
        "\n",
        "                d = torch.tensor([len(x)-1,s])\n",
        "                d = d.reshape(2,-1)\n",
        "                ei = torch.cat([ei, torch.tensor(d)], dim=-1)\n",
        "\n",
        "                d = torch.tensor([s,len(x)-1])\n",
        "                d = d.reshape(2,-1)\n",
        "                ei = torch.cat([ei, torch.tensor(d)], dim=-1)\n",
        "    \n",
        "    data.x = x\n",
        "    data.edge_index = ei \n",
        "    data.y = y\n",
        "    data.train_mask, data.val_mask, data.test_mask = train_mask, val_mask, test_mask\n",
        "\n",
        "    nodes = np.zeros(len(data.x))\n",
        "    for i in range(len(data.edge_index[0])):\n",
        "        a =  data.edge_index[0][i]\n",
        "        b =  data.edge_index[1][i]\n",
        "        nodes[a]+=1\n",
        "        nodes[b]+=1\n",
        "\n",
        "    return x,ei,y,d,strongs,nodes,data,train_mask,test_mask,val_mask,dataset,redunclass"
      ],
      "metadata": {
        "id": "utygo_YhzJ3q"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x,ei,y,d,strongs,nodes,data,train_mask,test_mask,val_mask,dataset,redunclass = make_data_ready(flag_type_2 = True,\n",
        "#                                                                                        each_part_size = 100,\n",
        "#                                                                                        istype2 = False)\n",
        "\n",
        "\n",
        "# _ = plt.hist(nodes, bins='auto')  # arguments are passed to np.histogram\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ra1IsQNOYqx1"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqWR0j_kIx67"
      },
      "source": [
        "Overall, this dataset is quite similar to the previously used [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) network.\n",
        "We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n",
        "For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n",
        "This results in a training node label rate of only 5%.\n",
        "\n",
        "In contrast to `KarateClub`, this graph holds the additional attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n",
        "Furthermore, we make use of **[data transformations](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-transforms) via `transform=NormalizeFeatures()`**.\n",
        "Transforms can be used to modify your input data before inputting them into a neural network, *e.g.*, for normalization or data augmentation.\n",
        "Here, we [row-normalize](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.NormalizeFeatures) the bag-of-words input feature vectors.\n",
        "\n",
        "We can further see that this network is undirected, and that there exists no isolated nodes (each document has at least one citation)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models running:"
      ],
      "metadata": {
        "id": "ejUlnVeFzna3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isfirst = True\n",
        "if isfirst:\n",
        "    log = {}\n",
        "    log['dataset']=datasetname\n",
        "    isfirst = False"
      ],
      "metadata": {
        "id": "ZNA9v24KVByK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = len(log.keys())\n",
        "\n",
        "each_part_size = 50\n",
        "flag_type_2 = False\n",
        "istype2 = False\n",
        "noiseratio = 0\n",
        "maxepoch = 101\n",
        "\n",
        "log['test'+str(index)] = {}\n",
        "log['test'+str(index)]['each_part_size'] = each_part_size\n",
        "log['test'+str(index)]['flag type 2'] = flag_type_2\n",
        "log['test'+str(index)]['istype2'] = istype2\n",
        "log['test'+str(index)]['noiseratio'] = noiseratio\n",
        "\n",
        "x,ei,y,d,strongs,nodes,data,train_mask,test_mask,val_mask,dataset,redunclass = make_data_ready(flag_type_2 = flag_type_2,\n",
        "                                                                                       each_part_size = each_part_size,\n",
        "                                                                                       istype2 = istype2,\n",
        "                                                                                       noiseratio = noiseratio) #flag_type_2 is connecting breaked nodes and istype2 means avoid breaked nodes in test "
      ],
      "metadata": {
        "id": "9D-YkE_32Njv"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###hide"
      ],
      "metadata": {
        "id": "wwfgmk5hWuNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 500})'''))\n",
        "\n",
        "\n",
        "#######\n",
        "##MLP##\n",
        "#######\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def mlp_train(model,data):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss,model\n",
        "\n",
        "def mlp_test(model,data):\n",
        "      model.eval()\n",
        "      out = model(data.x)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "def mlp_test2(model,data):\n",
        "    model.eval()\n",
        "    out = model(data.x)\n",
        "\n",
        "    true2 = 0\n",
        "    true3 = 0\n",
        "\n",
        "    acc1 = -1\n",
        "    acc2 = -1\n",
        "    acc3 = -1\n",
        "\n",
        "    #acc1: simple format\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "    acc1 =  test_acc\n",
        "\n",
        "    #acc2: weighted_avg_arg_max:\n",
        "    for clas in redunclass:\n",
        "        a = out[clas[0]]\n",
        "        for clasi in clas[1:]:\n",
        "            a += out[clasi]\n",
        "        pred = a.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true2 += 1\n",
        "    acc2 = true2/len(redunclass)\n",
        "\n",
        "    #acc3: voting:\n",
        "    for clas in redunclass:\n",
        "        votes = np.zeros(9)\n",
        "        \n",
        "        for clasi in clas:\n",
        "            a = out[clasi]\n",
        "            votes[a.argmax()]+=1\n",
        "        pred = votes.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true3 += 1\n",
        "    acc3 = true3/len(redunclass)\n",
        "\n",
        "    return acc1,acc2,acc3\n",
        "\n",
        "for epoch in range(1, maxepoch):\n",
        "    loss,model = mlp_train(model,data)\n",
        "    if epoch%20 == 0:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "print('\\n')\n",
        "acc1,acc2,acc3 = mlp_test2(model,data)\n",
        "log['test'+str(index)]['mlp'] = {}\n",
        "log['test'+str(index)]['mlp']['acc1']=f'{acc1:.4f}'\n",
        "log['test'+str(index)]['mlp']['acc2']=f'{acc2:.4f}'\n",
        "log['test'+str(index)]['mlp']['acc3']=f'{acc3:.4f}'\n",
        "\n",
        "\n",
        "#######\n",
        "##GNN##\n",
        "#######\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)\n",
        "\n",
        "# model.eval()\n",
        "# out = model(data.x, data.edge_index)\n",
        "# visualize(out, color=data.y)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def gnn_train(model,data):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss,model\n",
        "\n",
        "def gnn_test(model,data):\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "def gnn_test2(model,data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    #   true1 = 0\n",
        "    true2 = 0\n",
        "    true3 = 0\n",
        "\n",
        "    acc1 = -1\n",
        "    acc2 = -1\n",
        "    acc3 = -1\n",
        "\n",
        "    #acc1: simple format\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "    acc1 =  test_acc\n",
        "\n",
        "    #acc2: weighted_avg_arg_max:\n",
        "    for clas in redunclass:\n",
        "        a = out[clas[0]]\n",
        "        for clasi in clas[1:]:\n",
        "            a += out[clasi]\n",
        "        pred = a.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true2 += 1\n",
        "    acc2 = true2/len(redunclass)\n",
        "\n",
        "    #acc3: voting:\n",
        "    for clas in redunclass:\n",
        "        votes = np.zeros(9)\n",
        "        \n",
        "        for clasi in clas:\n",
        "            a = out[clasi]\n",
        "            votes[a.argmax()]+=1\n",
        "        pred = votes.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true3 += 1\n",
        "    acc3 = true3/len(redunclass)\n",
        "    return acc1,acc2,acc3\n",
        "\n",
        "for epoch in range(1, maxepoch):\n",
        "    loss,model = gnn_train(model,data)\n",
        "    if epoch%20==0:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "acc1,acc2,acc3 = gnn_test2(model,data)\n",
        "\n",
        "log['test'+str(index)]['gnn1'] = {}\n",
        "log['test'+str(index)]['gnn1']['acc1']=f'{acc1:.4f}'\n",
        "log['test'+str(index)]['gnn1']['acc2']=f'{acc2:.4f}'\n",
        "log['test'+str(index)]['gnn1']['acc3']=f'{acc3:.4f}'\n",
        "\n",
        "# model.eval()\n",
        "# out = model(data.x, data.edge_index)\n",
        "# visualize(out, color=data.y)\n",
        "\n",
        "\n",
        "########\n",
        "##GNN2##\n",
        "########\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, heads):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        # self.conv1 = GATConv(...)  # TODO\n",
        "        # self.conv2 = GATConv(...)  # TODO\n",
        "        # down:\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)  \n",
        "        self.conv2 = GATConv(hidden_channels, hidden_channels) \n",
        "        self.conv3 = GCNConv(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GAT(hidden_channels=16, heads=8)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def gnn2_train(model,data):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss,model\n",
        "\n",
        "\n",
        "def gnn2_test(model,data):\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "def gnn2_test2(model,data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    true2 = 0\n",
        "    true3 = 0\n",
        "\n",
        "    acc1 = -1\n",
        "    acc2 = -1\n",
        "    acc3 = -1\n",
        "\n",
        "    #acc1: simple format\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "    acc1 =  test_acc\n",
        "\n",
        "    #acc2: weighted_avg_arg_max:\n",
        "    for clas in redunclass:\n",
        "        a = out[clas[0]]\n",
        "        for clasi in clas[1:]:\n",
        "            a += out[clasi]\n",
        "        pred = a.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true2 += 1\n",
        "    acc2 = true2/len(redunclass)\n",
        "\n",
        "    #acc3: voting:\n",
        "    for clas in redunclass:\n",
        "        votes = np.zeros(9)\n",
        "        \n",
        "        for clasi in clas:\n",
        "            a = out[clasi]\n",
        "            votes[a.argmax()]+=1\n",
        "        pred = votes.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true3 += 1\n",
        "    acc2 = true3/len(redunclass)\n",
        "    return acc1,acc2,acc3\n",
        "\n",
        "for epoch in range(1, maxepoch):\n",
        "    loss,model = gnn2_train(model,data)\n",
        "    # val_acc = gnn2_test(data.val_mask)\n",
        "    test_acc = gnn2_test(model,data)\n",
        "    if epoch%20==0:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test: {test_acc:.4f}')\n",
        "\n",
        "print('\\n')\n",
        "acc1,acc2,acc3 = gnn_test2(model,data)\n",
        "\n",
        "log['test'+str(index)]['gnn2'] = {}\n",
        "log['test'+str(index)]['gnn2']['acc1']=f'{acc1:.4f}'\n",
        "log['test'+str(index)]['gnn2']['acc2']=f'{acc2:.4f}'\n",
        "log['test'+str(index)]['gnn2']['acc3']=f'{acc3:.4f}'\n",
        "\n",
        "##############\n",
        "##semi#light##\n",
        "##############\n",
        "\n",
        "class GCN_semi_light(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN_semi_light(hidden_channels=16)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def gnn_train(model,data):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss,model\n",
        "\n",
        "def gnn_test(model,data):\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      \n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "def semi_gnn_test2(model,data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    #   true1 = 0\n",
        "    true2 = 0\n",
        "    true3 = 0\n",
        "\n",
        "    acc1 = -1\n",
        "    acc2 = -1\n",
        "    acc3 = -1\n",
        "\n",
        "    #acc1: simple format\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "    acc1 =  test_acc\n",
        "\n",
        "    #acc2: weighted_avg_arg_max:\n",
        "    for clas in redunclass:\n",
        "        a = out[clas[0]]\n",
        "        for clasi in clas[1:]:\n",
        "            a += out[clasi]\n",
        "        pred = a.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true2 += 1\n",
        "    acc2 = true2/len(redunclass)\n",
        "\n",
        "    #acc3: voting:\n",
        "    for clas in redunclass:\n",
        "        votes = np.zeros(9)\n",
        "        \n",
        "        for clasi in clas:\n",
        "            a = out[clasi]\n",
        "            votes[a.argmax()]+=1\n",
        "        pred = votes.argmax()\n",
        "        if pred == data.y[clas[0]]:\n",
        "            true3 += 1\n",
        "    acc3 = true3/len(redunclass)\n",
        "\n",
        "    return acc1,acc2,acc3\n",
        "\n",
        "for epoch in range(1, maxepoch):\n",
        "    loss,model = gnn_train(model,data)\n",
        "    if epoch%20==0:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "\n",
        "acc1,acc2,acc3 = semi_gnn_test2(model,data)\n",
        "\n",
        "log['test'+str(index)]['semi gnn'] = {}\n",
        "log['test'+str(index)]['semi gnn']['acc1']=f'{acc1:.4f}'\n",
        "log['test'+str(index)]['semi gnn']['acc2']=f'{acc2:.4f}'\n",
        "log['test'+str(index)]['semi gnn']['acc3']=f'{acc3:.4f}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kmKr4COKGr6m",
        "outputId": "31ebc9e3-2030-4956-9efa-4c6734c4b6e2"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 500})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=3703, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=6, bias=True)\n",
            ")\n",
            "Epoch: 020, Loss: 1.5973\n",
            "Epoch: 040, Loss: 1.2095\n",
            "Epoch: 060, Loss: 0.8223\n",
            "Epoch: 080, Loss: 0.6115\n",
            "Epoch: 100, Loss: 0.6487\n",
            "\n",
            "\n",
            "GCN(\n",
            "  (conv1): GCNConv(3703, 16)\n",
            "  (conv2): GCNConv(16, 6)\n",
            ")\n",
            "Epoch: 020, Loss: 1.5970\n",
            "Epoch: 040, Loss: 1.2693\n",
            "Epoch: 060, Loss: 0.9667\n",
            "Epoch: 080, Loss: 0.8014\n",
            "Epoch: 100, Loss: 0.6462\n",
            "\n",
            "\n",
            "GAT(\n",
            "  (conv1): GCNConv(3703, 16)\n",
            "  (conv2): GATConv(16, 16, heads=1)\n",
            "  (conv3): GCNConv(16, 6)\n",
            ")\n",
            "Epoch: 020, Loss: 1.6309, Test: 0.5445\n",
            "Epoch: 040, Loss: 1.2550, Test: 0.6434\n",
            "Epoch: 060, Loss: 0.8317, Test: 0.6474\n",
            "Epoch: 080, Loss: 0.5805, Test: 0.6474\n",
            "Epoch: 100, Loss: 0.4643, Test: 0.6513\n",
            "\n",
            "\n",
            "GCN_semi_light(\n",
            "  (conv1): GCNConv(3703, 16)\n",
            "  (conv2): GCNConv(16, 6)\n",
            ")\n",
            "Epoch: 020, Loss: 1.4766\n",
            "Epoch: 040, Loss: 1.0599\n",
            "Epoch: 060, Loss: 0.7400\n",
            "Epoch: 080, Loss: 0.5618\n",
            "Epoch: 100, Loss: 0.4597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#logs:"
      ],
      "metadata": {
        "id": "s-t6Hvls4dUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log"
      ],
      "metadata": {
        "id": "sBbbI3HVlegD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870e76d6-1124-4967-cbb4-97091ced67ea"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset': 'CiteSeer',\n",
              " 'test1': {'each_part_size': 1000,\n",
              "  'flag type 2': False,\n",
              "  'istype2': False,\n",
              "  'noiseratio': 0,\n",
              "  'mlp': {'acc1': '0.5720', 'acc2': '0.5669', 'acc3': '0.5669'},\n",
              "  'gnn1': {'acc1': '0.7140', 'acc2': '0.7039', 'acc3': '0.7039'},\n",
              "  'gnn2': {'acc1': '0.6580', 'acc2': '0.6429', 'acc3': '0.6429'},\n",
              "  'semi gnn': {'acc1': '0.7040', 'acc2': '0.6958', 'acc3': '0.6958'}},\n",
              " 'test2': {'each_part_size': 100,\n",
              "  'flag type 2': False,\n",
              "  'istype2': False,\n",
              "  'noiseratio': 0,\n",
              "  'mlp': {'acc1': '0.5590', 'acc2': '0.5624', 'acc3': '0.5624'},\n",
              "  'gnn1': {'acc1': '0.7200', 'acc2': '0.7060', 'acc3': '0.7060'},\n",
              "  'gnn2': {'acc1': '0.6540', 'acc2': '0.6396', 'acc3': '0.6396'},\n",
              "  'semi gnn': {'acc1': '0.7050', 'acc2': '0.6937', 'acc3': '0.6937'}},\n",
              " 'test3': {'each_part_size': 50,\n",
              "  'flag type 2': False,\n",
              "  'istype2': False,\n",
              "  'noiseratio': 0,\n",
              "  'mlp': {'acc1': '0.5584', 'acc2': '0.5597', 'acc3': '0.5597'},\n",
              "  'gnn1': {'acc1': '0.7143', 'acc2': '0.7024', 'acc3': '0.7024'},\n",
              "  'gnn2': {'acc1': '0.6513', 'acc2': '0.6321', 'acc3': '0.6321'},\n",
              "  'semi gnn': {'acc1': '0.7023', 'acc2': '0.6916', 'acc3': '0.6916'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtSj7Z422MRO"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}